{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95353eb8-7d8c-4072-bb4c-6f9d83fbd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdd1c8d-efc6-4e9b-b4ee-c0e4d840240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62baed9b-25b2-4713-a26c-ec93883dcc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job Id', 'Experience', 'Qualifications', 'Salary Range', 'location',\n",
       "       'Country', 'latitude', 'longitude', 'Work Type', 'Company Size',\n",
       "       'Job Posting Date', 'Preference', 'Contact Person', 'Contact',\n",
       "       'Job Title', 'Role', 'Job Portal', 'Job Description', 'Benefits',\n",
       "       'skills', 'Responsibilities', 'Company', 'Company Profile'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27533bfd-554b-4c4a-a4bf-21e37fb93ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Company Profile'],inplace=True)\n",
    "df.drop(columns=['Contact'],inplace=True)\n",
    "df.drop(columns=['Contact Person'],inplace=True)\n",
    "df.drop(columns=['Job Id'],inplace=True)\n",
    "df.drop(columns=['Job Posting Date'],inplace=True)\n",
    "columns_to_drop = ['latitude', 'longitude', 'Job Portal']\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b19430-2216-495b-a1e4-1d20f32fb0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Experience', 'Qualifications', 'Salary Range', 'location', 'Country',\n",
       "       'Work Type', 'Company Size', 'Preference', 'Job Title', 'Role',\n",
       "       'Job Description', 'Benefits', 'skills', 'Responsibilities', 'Company'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564e2acc-ae52-4b0d-bd88-462aaad31ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Min Salary', 'Max Salary']] = df['Salary Range'].str.extract(r'\\$?(\\d+)[kK]-\\$?(\\d+)[kK]')\n",
    "\n",
    "# Convert extracted values to numeric and multiply by 1000\n",
    "df['Min Salary'] = pd.to_numeric(df['Min Salary']) * 1000\n",
    "df['Max Salary'] = pd.to_numeric(df['Max Salary']) * 1000\n",
    "\n",
    "# Optionally, you can create an average salary column\n",
    "df['Average Salary'] = (df['Min Salary'] + df['Max Salary']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3da38aad-b88c-4812-9dcd-4262cbef163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# col = ['Experience', 'Qualifications', 'location', 'Country',\n",
    "#        'Work Type', 'Company Size', 'Preference', 'Job Title', \n",
    "#        'Role', 'Benefits', \n",
    "#        'Responsibilities', 'Company']\n",
    "\n",
    "# encoding_dict={}\n",
    "\n",
    "# # Loop through each categorical column and apply LabelEncoder\n",
    "# for c in col:\n",
    "#     df[c] = le.fit_transform(df[c].astype(str))  \n",
    "#     encoding_dict[c] = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c26070d1-5f2f-48e8-ab46-a38295fd2c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Experience', 'Qualifications', 'location', 'Country', 'Work Type', 'Company Size', 'Preference', 'Job Title', 'Role', 'Benefits', 'Responsibilities', 'Company'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b12bafad-0eae-44ee-87aa-06e8616c9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample job descriptions\n",
    "job_descriptions = df['Job Description'].tolist()\n",
    "\n",
    "# Print original job descriptions\n",
    "# print(\"Original Job Descriptions:\")\n",
    "# print(job_descriptions)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Ensure that the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing non-alphabetical characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Debugging: Print tokens after removing stopwords\n",
    "    # print(f\"Tokens after stopword removal: {tokens}\")  # Debugging line\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the processed text\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    # Debugging: Print processed text for each job description\n",
    "    # print(f\"Processed text: '{processed_text}'\")  # Debugging line\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocessing to the 'Job Description' column\n",
    "job_descriptions = df['Job Description'].apply(preprocess_text).tolist()\n",
    "\n",
    "# Filter out empty descriptions\n",
    "job_descriptions = [desc for desc in job_descriptions if desc]  # Keep only non-empty descriptions\n",
    "\n",
    "# Print the final job descriptions after preprocessing\n",
    "print(\"Final Job Descriptions after Preprocessing:\")\n",
    "print(job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "868bed81-32da-4cdb-8a21-80a35e0d7028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1615940"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c75dd294-3ee2-4035-9daa-c9bb92e55d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cde0a50-2c4c-4a93-9b1e-6214648802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise all job descriptions\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_data = vectorizer.fit_transform(job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64f826d3-8efe-4633-8be6-566fd97daf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software development\n",
      "most_similar_idx 180\n",
      "Best matched job: 14\n",
      "Job title 110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def match_job_description(user_input):\n",
    "    print(user_input)\n",
    "    # Preprocess the user input\n",
    "    user_input_processed = preprocess_text(user_input)\n",
    "    \n",
    "    # Vectorize the user input\n",
    "    user_vector = vectorizer.transform([user_input_processed])\n",
    "    # Compute cosine similarity with job descriptions\n",
    "    similarities = cosine_similarity(user_vector, vectorized_data)\n",
    "    \n",
    "    # Get the index of the most similar job description\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    print(\"most_similar_idx\", most_similar_idx)\n",
    "    \n",
    "    # Return the corresponding job title\n",
    "    return df['Job Title'].iloc[most_similar_idx]\n",
    "\n",
    "# Example usage\n",
    "user_input = \"software development\"\n",
    "matched_job = match_job_description(user_input)\n",
    "\n",
    "print(f\"Best matched job: {matched_job}\")\n",
    "print('Job title',encoding_dict['Job Title'][matched_job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "320dfaca-dbe2-47e4-98b9-b85ec638ccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Job Title'][49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdd21198-72f2-4158-89cc-c47fb86d84c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'142'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict['Job Title'][49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fabd2f6a-9e7e-490c-97f1-0d70b36458f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Experience', 'Qualifications', 'location', 'Country', 'Work Type', 'Company Size', 'Preference', 'Job Title', 'Role', 'Benefits', 'Responsibilities', 'Company'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe05afa0-acec-4620-8e5d-4b613055d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills = df_test['skills'].tolist()\n",
    "\n",
    "# Apply the preprocessing to the 'Job Description' column\n",
    "skills_processed = df['skills'].apply(preprocess_text).tolist()\n",
    "\n",
    "# Filter out empty descriptions\n",
    "skills_processed = [desc for desc in skills_processed if desc]  # Keep only non-empty descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e26474d-59cb-4f88-b25b-10f354bda63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skills_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc293107-01d4-427c-979d-5048be30d76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer_skills \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 2\u001b[0m vectorized_data \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer_skills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskills_processed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1280\u001b[0m         )\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vectorizer_skills = TfidfVectorizer()\n",
    "vectorized_data = vectorizer_skills.fit_transform(skills_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b0edc-0955-4e3a-b51f-a8f5b5fa4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def match_skills(user_input):\n",
    "    print(user_input)\n",
    "    # Preprocess the user input\n",
    "    user_input_processed = preprocess_text(user_input)\n",
    "    \n",
    "    # Vectorize the user input\n",
    "    user_vector = vectorizer_skills.transform([user_input_processed])\n",
    "    # Compute cosine similarity with job descriptions\n",
    "    similarities = cosine_similarity(user_vector, vectorized_data)\n",
    "    \n",
    "    # Get the index of the most similar job description\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    print(\"most_similar_idx\", most_similar_idx)\n",
    "    print(\"Company\", encoding_dict['Company'][df['Company'].iloc[most_similar_idx]])\n",
    "    # df.iloc[most_similar_idx],\n",
    "    # Return the corresponding job title\n",
    "    return df['Job Title'].iloc[most_similar_idx]\n",
    "\n",
    "# Example usage\n",
    "user_input = \"nodejs, react, mobile development\"\n",
    "matched_job = match_skills(user_input)\n",
    "\n",
    "print(f\"Best matched job: {matched_job}\")\n",
    "print('Job title: ', encoding_dict['Job Title'][matched_job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7d2e3-e7fd-462e-9398-2faf8418eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7f2babf-9c6c-447b-9ab2-064eacc1b300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    307\n",
       "1    154\n",
       "2    269\n",
       "3    374\n",
       "4    126\n",
       "Name: skills, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df['skills'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e3b4e-9437-4053-9342-e1443bbcf7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
